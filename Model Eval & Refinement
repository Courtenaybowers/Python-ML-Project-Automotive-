#In-sample evaluation tells us how well our model will fit the data used to train it, but how well does the model predict new data?
  #Solution = split the data up, use in-sample data (training data) to train the model, the rest of the data (test data/ out-of-sample data)

#Split the data into random train and test subsets
from sklearn.model_selection import train_test_split


#Apply cross validation
from sklearn.model_selection import cross_val_score

scores=cross_val_score(lr,x_data,y_data,cv=3)

np.mean(scores)


#
sklearn.model_selection import cross_val_predict
yhat=cross_val_predict(lr2e,x_data,y_data,cv=3)


#The goal of model selection is to determine the order of the polynomial to provide the best estimate of the function y(x)+noise.
  #Underfitting: where the model is to simple to fit the data (linear model, but line is not complex enough "polynomial would be better")
  #Overfitting: where the model is too flexible and fits the noise rather than the function






