#Import model and create object
from sklearn.linear_model import LinearRegression
lm=LinearRegression()

--------------------------------------------------------Linear Regression----------------------------------------------------------
#What is the predicted value of a car based on highway-mpg?
x = df[['highway-mpg']]
Y = df[['price']]
lm.fit(X,Y)
Yhat=lm.predict(X)

lm.intercept_
lm.coef_

print("Price = ",'lm.intercept_','lm.coef_',"*",highway_mpg)

--------------------------------------------------------Multiple Linear Regression----------------------------------------------------------
#Does 'horsepower','curb-weight','engine-size','highway-mpg' predict the target ('price' of a car)?
    #Define the independent variables, then train the model with the dependent variable, then obtain a prediction)
z = df[['horsepower','curb-weight','engine-size','highway-mpg']]
lm.fit(z, df['price'])
Yhat=lm.predict(X)

lm.intercept_
lm.coef_

#THE ESTIMATED LINEAR MODEL:
#Price = lm.intercept + b1*horsepower + b2*curb-weight + b3*engine-size + b4*highway-mpg



-----------------------------------------Plot the models (regression, residual, distribution)-----------------------------------------------------
#Plot the relationship between y and X (Dependent variable/feature of "price")
import seaborn as sns

sns.reglot.(x="highway-mpg",y="price", data=df)
plt.ylim(0,)


#Create a residual plot
import seaborn as sns
sns.residplot.(df['highway-mpg'],df['price'])


#Create a distribution plot (if want histogram = change "False", 
import seaborn as sns

ax-1 = sns.distplot(of['price'], hist=False, color-"r", label-"Actual Value")

sns.distplot(Yhat, hist-False, color-"b", label-"Fitted Values", ax-axl)
 

-----------------------------------------Polynomial Regressions-----------------------------------------------------
#Calculation uses Polynomial of 3rd order
f=np.polyfit(x,y,3)
p=np.polyld(f)

print(p)

#Polynomial regression with more than one dimension
from sklearn.preprocessing import PolynomialFeatures
pr=PolynomialFeatures(degree=2, include_bias=False)

    #transform features into polynomial feature
x_polly-pr,fit_transform(x(['horsepower', 'curb-weight'])

#EXAMPLE: (Normalize features simultaneously)
from sklearn.preprocessing import StandardScaler

Scale=StandardScaler()
Scale.fit(x_data[['horsepower', 'highway-mpg']])

x_scale=SCALE.transform(x_data[['horsepower, 'highway-mpg']])


#-------------------------------------Pipeline EXAMPLE (Normalization => Polynomial transform => Linear regression)----------------------------------
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

from sklearn.pipeline import Pipeline

Input=[('scale',StandardScaler()),('polynomial',PolynomialFeatures(degree=2),('model',LinearRegression())]

pipe=Pipeline(Input)

Pipe.train(X['horsepower', 'curb-weight', 'engine-size', 'highway-mpg'],y)

yhat=Pipe.predict(x[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])


#-------------------------------------------------Measures of Model Evaluation----------------------------------------------------------
    #Important measures to determine the fit of a model:
        #Mean Squared Error (MSE).....(
from sklearn.metrics import mean_squared_error
mean_squared_errors(df['price'],y_predict_simple_fit)

        #R-squared (R^2) or the Coefficient of Determination....(the closer to 1 the better, "0" means the output is as good as just using the avg of the data)
X = df[['highway-mpg']]
Y = df[['price']]

lm.fit(X, Y)



#-----------------------EXAMPLE Prediction ()---------------------------
    #Train the model, then predict the price
lm.fit(df['highway-mpg'],df['prices']))
lm.predict(30)

    #regression equation seems reasonable. Now generate a sequence of values in a specified range.
        #ARRANGE function (starting point, end point +1, step size between elements
import numpy as np
new_input_np.arange (1,101,1).reshape(-1,1)

yhat=lm.predict(new_input)







#-------------------------------------------------MODEL DEVELOPMENT (CODE)----------------------------------------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

path = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/automobileEDA.csv'
df = pd.read_csv(path)
df.head()

#Linear Regressions (import and create object)
from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm

X = df[['highway-mpg']]
Y = df['price']

#Fit the model and output a prediction
lm.fit(X,Y)

Yhat=lm.predict(X)
Yhat[0:5]

#What is the value of the intercept and the value of the slope?
lm.intercept_
    38423.3058581574
lm.coef_
    array([-821.73337832])

#"Yhat = a +bX"
#price = 38423.31 - 821.73 x highway-mpg

#EXAMPLE lm1:
lm1 = LinearRegression()
lm1

X1 = df[['engine-size']]
Y1 = df[['price']]
lm1.fit(X1,Y1)

lm1.coef_

lm1.intercept_

    #"Yhat= lm1.intercept_ + lm1.coef_*X1


#Multiple Linear Regression (Develop a model using the predictor variables below "Z", fit linear model)
    #(Yhat = a + b1X1 + b2X2 + b3X3 +b4X4)
Z = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]

lm.fit(Z, df['price'])

lm.intercept_

lm.coef_

    #"Price = -15678.742628061467 + 52.65851272 x horsepower + 4.69878948 x curb-weight + 81.95906216 x engine-size + 33.58258185 x highway-mpg"

#EXAMPLE
Z1 = df[['normalized-losses', 'highway-mpg']]
lm2 = LinearRegression()
lm2.fit(Z1, df['price']])

    #or
lm2 = LinearRegression()
lm2.fit(df[['normalized-losses', 'highway-mpg']], df['price'])

lm2.coef_

lm2.intercept_


#MODEL EVALUATION/Visualization (plot regression)
import seaborn as sns
%matplotlib inline

width = 12
height = 10
plt.figure(figsize=(width, height))
sns.regplot(x="highway-mpg", y="price", data=df)
plt.ylim(0,)

    #EXAMPLE (visualize and verify relationship using ".corr()")
import seaborn as sns
%matplotlib inline

pltfigure(figsize=(width, height))
sns.regplot(x="peak-rpm", y="price", data=df)
plt.ylim(0,)


df[["peak-rpm", "highway-mpg", "price"]].corr()


    #Example visualize the variance of the data (Residual plot)...(Residual = the difference betwee nthe oberserved y and the predicted Yhat)=difference b/n data point and regression line
width = 12
height = 10
plt.figure(figsize=(width, height))
sns.residplot(df['highway-mpg'], df['price'])
plt.show()


    #EXAMPLE (Multiple Linear Regression)
Y_hat = lm.predict(Z)

plt.figure(figsize=(width, height))

ax1 = sns.distplot(df['price'], hist=False, color="r", label="Actual Value")
sns.distplot(Y_hat, hist=False, color="b", label="Fitted Values" , ax=ax1)


plt.title('Actual vs Fitted Values for Price')
plt.xlabel('Price (in dollars)')
plt.ylabel('Proportion of Cars')

plt.show()
plt.close()


    #EXAMPLE (Polynomial Regression & Pipelines)
        #Quadratic - 2nd order
            Yhat = a + b1X + b2X2
        #Cubic - 3rd order
            Yhat = a + b1X + b2X2 + b3X3
        #Higher order
            Y = a + b1X + b2X2 + b3X3
            
def PlotPolly(model, independent_variable, dependent_variabble, Name):
    x_new = np.linspace(15, 55, 100)
    y_new = model(x_new)

    plt.plot(independent_variable, dependent_variabble, '.', x_new, y_new, '-')
    plt.title('Polynomial Fit with Matplotlib for Price ~ Length')
    ax = plt.gca()
    ax.set_facecolor((0.898, 0.898, 0.898))
    fig = plt.gcf()
    plt.xlabel(Name)
    plt.ylabel('Price of Cars')

    plt.show()
    plt.close()

    #Get the variables, fit the polynomial to 3rd order "cubic" using polyfit and poly1d to display the function, plot the function
x = df['highway-mpg']
y = df['price']

f = np.polyfit(x, y, 3)
p = np.poly1d(f)
print(p)

PlotPolly(p, x, y, 'highway-mpg')

np.polyfit(x, y, 3)

    #EXAMPLE (polynomial transform on multiple features)
from sklearn.preprocessing import PolynomialFeatures

        #create a PolynomialFeatures object of degree 2
pr = PolynomialFeatures(degree=2)
pr

Z_pr=pr.fit_transform(Z)

Z.shape()

Z_pr.shape


#Create a Pipeline (simplifies the processing of data)
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

    #create the Pipeline, by creating a list of tuples including the name of the model or estimator and its corresponding constructor
Input=[('scale', StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model', LinearRegression())]

pipe=Pipeline(Input)
pipe

    #normalize the data, perform the transformation and fit the model simultaneously
pipe.fit(Z,y)

    #OR normalize, perform the transformation, and produce prediction simultaneously
ypipe=pipe.predict(Z)
ypipe[0:4]

    #EXAMPLE pipeline (standardize the data, perform the prediction using LinearRegression using Z and target y)
Input=[('scale', StandardScaler()),('model', LinearRegression())]
pipe=Pipeline(Input)
pipe.fit(Z,y)
ypipe=pipe.predict(Z)
ypipe[0:10]



#Measuring the In-Sample Evaluation
    #R^2 or Coefficient of Determination
    #Mean Squared Error (MSE)










